<!DOCTYPE html>
<html lang="chinese (simplified)">

<head>
            <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">


        <title>RNN和LSTM的理解</title>

            <link href="https://zeng-yan.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Zeng Yan's Blog Full Atom Feed" />
            <link href="https://zeng-yan.github.io/feeds/{slug}.atom.xml" type="application/atom+xml" rel="alternate" title="Zeng Yan's Blog Categories Atom Feed" />
        <!-- Bootstrap Core CSS -->
        <link href="https://zeng-yan.github.io/theme/css/bootstrap.min.css" rel="stylesheet">

        <!-- Custom CSS -->
        <link href="https://zeng-yan.github.io/theme/css/clean-blog.min.css" rel="stylesheet">

        <!-- Code highlight color scheme -->
            <link href="https://zeng-yan.github.io/theme/css/code_blocks/monokai.css" rel="stylesheet">


        <!-- Custom Fonts -->
        <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
        <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
        <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

        <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
        <![endif]-->



        <meta name="description" content="RNN 现在我们的数据样本是这样的序列的形式： $$ [x_1,x_2,...,x_t,..., x_n] $$ 序列中的每个元素都不是独立的，而是和其他元素存在着一定联 …">

        <meta name="author" content="Zengyan">

        <meta name="tags" content="算法">
        <meta name="tags" content="神经网络">
        <meta name="tags" content="Python">

	                <meta property="og:locale" content="">
		<meta property="og:site_name" content="Zeng Yan's Blog">

	<meta property="og:type" content="article">
            <meta property="article:author" content="https://zeng-yan.github.io/author/zengyan.html">
	<meta property="og:url" content="https://zeng-yan.github.io/rnn-and-lstm.html">
	<meta property="og:title" content="RNN和LSTM的理解">
	<meta property="article:published_time" content="2021-09-13 00:00:00+08:00">
            <meta property="og:description" content="RNN 现在我们的数据样本是这样的序列的形式： $$ [x_1,x_2,...,x_t,..., x_n] $$ 序列中的每个元素都不是独立的，而是和其他元素存在着一定联 …">

            <meta property="og:image" content="https://zeng-yan.github.io/images/andreas-rocha-castleonahill01.jpg">
    <link rel="icon" href="/images/favicon.ico">
</head>

<body class="article-rnn-and-lstm">

    <!-- Navigation -->
    <nav class="navbar navbar-default navbar-custom navbar-fixed-top">
        <div class="container-fluid">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand" href="https://zeng-yan.github.io/">Zeng Yan's Blog</a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">

                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

    <!-- Page Header -->
        <header class="intro-header" style="background-image: url('https://zeng-yan.github.io//images/andreas-rocha-castleonahill01.jpg')">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    <div class="post-heading">
                        <h1>RNN和LSTM的理解</h1>
                        <span class="meta">Posted by
                                <a href="https://zeng-yan.github.io/author/zengyan.html">Zengyan</a>
                             on 周一 13 九月 2021
                        </span>
                        
                    </div>
                </div>
            </div>
        </div>
    </header>

    <!-- Main Content -->
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
    <!-- Post Content -->
    <article>
        <h3 id="rnn">RNN</h3>
<p>现在我们的数据样本是这样的序列的形式：</p>
<div class="math">$$
[x_1,x_2,...,x_t,..., x_n]
$$</div>
<p>序列中的每个元素都不是独立的，而是和其他元素存在着一定联系，例如一个句字就是这种形式，句子中的每个词和其他词是存在上下文关系的。现在我们要对这个句子建模，捕捉它整体的含义，那我们的模型就必须要考虑这种数据结构中元素和元素间的关联。为了处理这样的序列数据，RNN应运而生，它的一般形式如下：</p>
<div class="math">$$
y_t, h_{t} = f(x_t, h_{t-1})
$$</div>
<p>输入有两个，x和隐藏状态h；输出有两个，y和下一个时间步的隐藏状态h。RNN并不是一次性把整个序列输入，而是每次按时间步t把序列中元素依次输入，这样的做法可以让模型处理每个样本序列长度不一的情况。RNN的关键就是这个隐藏状态h，每个时间步结合上一步的h和当前的x更新h，当前x的信息和前文的信息被整合进h，然后传入下一时间步，实现了对前文的记忆。</p>
<p>通常来说，RNN可以用下面这样的形式实现，将来自上个时间步的h和x做拼接然后用全连接层映射。注意h到y的映射不是必须的，有的实现中直接用h作为y，这一点很好理解，你确实需要的话，对h用一层全连接映射到y就是了，这种额外的操作是容易实现的。</p>
<div class="math">$$
h_t = \sigma(W_a [h_{t-1}, x_t] + b) \\
y_t = \sigma(W_b h_t + b)
$$</div>
<p>尽管RNN被设计用来记忆信息，但它的记忆力实在是不怎么样，观察RNN的输出，它经过激活函数映射到(0,1)，在多步运算后数值衰减很快，也就是学习的信息在几步运算后就趋于零了。那我们不用激活函数呢？且不提激活函数引入非线性性的事，激活函数可以把输出映射到一定的区间范围内，假设撇掉x，计算h时没有激活函数，h在多步运算后像下面这样，若w的特征值小于1，h100几乎全为0，w特征值大于1，h100数值爆炸。</p>
<div class="math">$$
h_{100} = Wh_{99} = W^{2}h_{98} ... = W^{100}h_0
$$</div>
<p>从反向传播的角度来看RNN也是不行，同一层神经元在求导时共享权重矩阵，多个时间步的计算后，梯度从后向前一通链式法则连乘过来，求导时雅可比矩阵特征值小于1，梯度消失，特征值大于1，梯度爆炸。梯度爆炸还可以裁剪梯度，梯度消失就很头疼了。</p>
<p>综上所述，RNN无法很好地处理长程依赖。</p>
<h3 id="lstm">LSTM</h3>
<p>为了解决RNN在长序列训练过程中的梯度消失和梯度爆炸问题，LSTM诞生了。LSTM仍然是一种RNN，不过在计算的时候增加了一些额外的操作。RNN只有一个传递状态h，LSTM有两个传递状态h和c，RNN中的h对应LSTM中的c。LSTM中c改变慢，通常是上一个状态传递过来加上一些数值；h在不同节点下改变明显。</p>
<div class="math">$$
y_t,(h_t,c_t) = f(x_t, (h_{t-1},c_{t-1}))
$$</div>
<p>来看LSTM具体的计算，同样先将上一个时间步的h和当前输入x拼接，但用全连接层计算出四个状态：</p>
<div class="math">$$
\begin{cases}
z = tanh(W_z [x_t,h_{t-1}] + b). \quad  (-1,1) \\
z_i = sigmoid(W_i [x_t, h_{t-1}] + b) \quad (0,1) \\
z_f = sigmoid(W_f [x_t, h_{t-1}] + b) \quad (0,1) \\
z_o = sigmoid(W_o [x_t, h_{t-1}] + b) \quad (0,1) \\
\end{cases}
$$</div>
<p>后面三个介于0和1之间的状态作为一种门控状态。它们的作用类似于权重系数，控制信息通过或屏蔽的程度。这些门控状态全是x和h拼接到一起计算得到的，不是单独看x或者h，因为你是根据当前的输入和前面的历史信息一起来决定哪些重要哪些不重要的。然后，LSTM的计算分为几个阶段：</p>
<ul>
<li>
<p>忘记阶段：用zf控制上一个传递状态c哪些需要留下哪些需要忘记。</p>
</li>
<li>
<p>选择记忆阶段：用zi控制选择，对当前输入x选择记忆，哪些重要哪些不重要。在此之前还有一件事，先把x处理成z，为啥要这么做？因为你得把x弄成和zi一样的形状然后再选择。</p>
</li>
</ul>
<p>此时，传递状态和输入都被处理了，你可以来更新传递状态了。</p>
<ul>
<li>输出阶段：用zo控制，哪些被当前状态输出，以及用tanh缩放。</li>
</ul>
<p>用黑话翻译一下上面的过程（⊙是元素级乘法）：</p>
<div class="math">$$
\begin{cases}
c_t = z_f \odot c_{t-1} + z_i \odot z \\
h_t = z_o \odot tanh(c_t) \\
y_t = sigmoid(W h_t + b)
\end{cases}
$$</div>
<p>同样的，h到y的映射不是必须的，有的实现中直接用h作为y，Pytorch中就是如此。Pytorch中LSTM的具体计算实现如下：</p>
<p><img alt="Pytorch的RNN" src="https://zeng-yan.github.io/essayimages/torch-rnn.png"></p>
<p>这里x和h分别和对应的w相乘在相加，实际上和上面写的x和h拼接后再乘w的形式是一样的，但注意Pytorch每个状态的计算用了两个偏置项。所以如下形式LSTM的参数量为：</p>
<div class="math">$$
5376 = 第一层LSTM参数量3200 + 第二层LSTM参数量2176 \\
= (hidden\_size*input\_size + bias\_size + hidden\_size*hidden\_size+ bias\_size)*4 + ... \\
= (16*32+16+16*16+16)*4 + (16*16+16+16*16+16)*4
$$</div>
<div class="highlight"><pre><span></span><code><span class="n">lstm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div>

<p>Pytorch中LSTM输入是<code>(input, (h_0, c_0))</code> 的形式，<code>(h_0, c_0)</code>默认设置为零向量。输出是<code>(output, (h_n, c_n))</code>的形式，当有多层LSTM时，这里<code>output</code>是最后一层LSTM每个时间步的h的集合<code>[h_1,h_2,...,h_n]</code>，<code>(h_n, c_n)</code>是每层LSTM最后一个时间步的h和c。通过下面这段简单的代码可以理解pytorch中LSTM的输入输出和参数量：</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">32</span><span class="p">))</span>  <span class="c1"># [n_seq, seq_length, n_feature]</span>
<span class="n">lstm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">c</span><span class="p">))</span> <span class="o">=</span> <span class="n">lstm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">lstm</span><span class="o">.</span><span class="n">parameters</span><span class="p">()))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>  <span class="c1"># [n_seq, seq_length, hidden_size*n_direction]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">h</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>  <span class="c1"># [n_layer*n_direction, n_seq, hidden_size]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>  <span class="c1"># [n_layer*n_direction, n_seq, hidden_size]</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="mi">5376</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">16</span><span class="p">])</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">16</span><span class="p">])</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">16</span><span class="p">])</span>
</code></pre></div>

<h3 id="rnn_1">双向RNN</h3>
<p>我们可以分别构建两个RNN分别从左向右，和从右向左，各自输出自己的状态向量，然后拼接起来：</p>
<p><img alt="双向RNN" src="https://zeng-yan.github.io/essayimages/bi-rnn.png"></p>
<p>双向RNN比单向RNN表现好的原因：</p>
<ul>
<li>
<p>减轻对前面记忆的遗忘：从左向右的RNN输出的h可能遗忘掉左端的信息，从右向左的RNN输出的h可能遗忘掉右端的信息，把两者结合到一起就能不足对方遗忘的信息。</p>
</li>
<li>
<p>补足后文的信息：对序列中某个元素的理解可能不仅仅依靠前面的信息，也需要借助后文，所以单方向的移动可能是不够的。</p>
</li>
</ul>
<p>在Pytorch中实现双向的LSTM只需要设置参数<code>bidirectional=True</code>即可：</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">32</span><span class="p">))</span>  <span class="c1"># [n_seq, seq_length, n_feature]</span>
<span class="n">lstm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">c</span><span class="p">))</span> <span class="o">=</span> <span class="n">lstm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">lstm</span><span class="o">.</span><span class="n">parameters</span><span class="p">()))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>  <span class="c1"># [n_seq, seq_length, hidden_size*n_direction]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">h</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>  <span class="c1"># [n_layer*n_direction, n_seq, hidden_size]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>  <span class="c1"># [n_layer*n_direction, n_seq, hidden_size]</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="mi">12800</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">32</span><span class="p">])</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">16</span><span class="p">])</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">16</span><span class="p">])</span>
</code></pre></div>

<div class="math">$$
12800 = 第一层LSTM参数量6400 + 第二层LSTM参数量6400 \\
= (16*32+16+16*16+16)*4*2 + (16*32+16+16*16+16)*4*2
$$</div>
<h2 id="refer">REFER</h2>
<blockquote>
<p><a href="https://www.jianshu.com/p/4b4701beba92">人人都能看懂的LSTM</a></p>
<p><a href="https://space.bilibili.com/1369507485?spm_id_from=333.788.b_765f7570696e666f.1">RNN模型与NLP应用</a></p>
<p><a href="https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html">torch.nn.LSTM</a></p>
</blockquote>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
    </article>

        <div class="tags">
            <p>tags: <a href="https://zeng-yan.github.io/tag/suan-fa.html">算法</a>, <a href="https://zeng-yan.github.io/tag/shen-jing-wang-luo.html">神经网络</a>, <a href="https://zeng-yan.github.io/tag/python.html">Python</a></p>
        </div>

    <hr>

            </div>
        </div>
    </div>

    <hr>

    <!-- Footer -->
    <footer>
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    <ul class="list-inline text-center">
                            <li>
                                <a href="https://github.com/Zeng-Yan">
                                    <span class="fa-stack fa-lg">
                                        <i class="fa fa-circle fa-stack-2x"></i>
                                        <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a>
                            </li>
                    </ul>
<p class="copyright text-muted">
    Blog powered by <a href="http://getpelican.com">Pelican</a>,
    which takes great advantage of <a href="http://python.org">Python</a>. <br />        &copy;  Zengyan
</p>                </div>
            </div>
        </div>
    </footer>

    <!-- jQuery -->
    <script src="https://zeng-yan.github.io/theme/js/jquery.min.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="https://zeng-yan.github.io/theme/js/bootstrap.min.js"></script>

        <!-- Custom Theme JavaScript -->
        <script src="https://zeng-yan.github.io/theme/js/clean-blog.min.js"></script>

</body>

</html>